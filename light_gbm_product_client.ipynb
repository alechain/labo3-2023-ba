{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_mas_cols = df.filter(like='tn_mas_1').columns\n",
    "print(tn_mas_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.periodo.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to numerical format\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = pd.Categorical(df[col]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/home/all4data118/buckets/b1/datasets/df_final_sellout_prod_client_train_fe_es_b.parquet')\n",
    "tn_mas_cols = df.filter(like='tn_mas_1').columns\n",
    "print(tn_mas_cols)\n",
    "\n",
    "# train y test\n",
    "# train y test\n",
    "df=df[df.customer_id.isin([10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009, 10011, 10012, 10013] )]\n",
    "df=df[df.product_id.isin([20001, 20002, 20003, 20004, 20005, 20006, 20007, 20009, 20011, 20032] )]\n",
    "df=df.reset_index(drop=True)\n",
    "X_train=df[(df.periodo>=201801)&(df.periodo<=201908)]\n",
    "X_test=df[(df.periodo==201909)]\n",
    "Y_train=df[(df.periodo>=201801)&(df.periodo<=201908)][['id_row','tn_mas_2']]\n",
    "Y_test=df[(df.periodo==201909)][['id_row','tn_mas_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.drop(['id_row','tn_mas_2'],axis=1)\n",
    "X_test=X_test.drop(['id_row','tn_mas_2'],axis=1)\n",
    "\n",
    "Y_train_sc=Y_train[['tn_mas_2']]\n",
    "Y_test_sc=Y_test[['tn_mas_2']]\n",
    "# Ensure that the number of labels matches the number of data points\n",
    "assert len(X_train) == len(Y_train_sc), \"Mismatch in the number of labels for training set\"\n",
    "assert len(X_test) == len(Y_test_sc), \"Mismatch in the number of labels for validation set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features based on their data types\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "# Create a dictionary to store the label encoders\n",
    "label_encoders = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    label_encoder = LabelEncoder()\n",
    "    X_train[feature] = label_encoder.fit_transform(X_train[feature])\n",
    "    X_test[feature] = label_encoder.transform(X_test[feature])\n",
    "    \n",
    "    # Save the label encoder to the dictionary\n",
    "    label_encoders[feature] = label_encoder\n",
    "\n",
    "# Save the label encoders to a file using joblibbuko118/exp/model_es_b/client_product\n",
    "joblib.dump(label_encoders, '/home/all4data118/buckets/b1/exp/model_es_b/client_product/label_encoders.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para subir depsues\n",
    "# Load the label encoders from the file\n",
    "label_encoders = joblib.load('/home/all4data118/buckets/b1/exp/model_es_b/client_product/label_encoders.joblib')\n",
    "\n",
    "# Assuming X_train and X_test are your datasets and categorical_features is the list of categorical feature names\n",
    "\n",
    "# Apply the label encoders to the respective features\n",
    "for feature, label_encoder in label_encoders.items():\n",
    "    X_train[feature] = label_encoder.transform(X_train[feature])\n",
    "    X_test[feature] = label_encoder.transform(X_test[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have your data in X_train, Y_train_sc, X_test, and Y_test_sc\n",
    "\n",
    "# Define your custom metric function (MAPE)\n",
    "def mape(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    absolute_percentage_errors = np.abs((y_true - y_pred) / y_true)\n",
    "    return 'MAPE', np.mean(absolute_percentage_errors) * 100, False\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=Y_train['tn_mas_2_scaled'], categorical_feature=categorical_features)\n",
    "val_data = lgb.Dataset(X_test, label=Y_test['tn_mas_2_scaled'], categorical_feature=categorical_features)\n",
    "\n",
    "# Define LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "  #  'metric': 'l1',  # Using 'l1' as the default metric for early stopping\n",
    "   # 'boosting_type': 'gbdt',\n",
    "   # 'num_leaves': 31,\n",
    "   # 'learning_rate': 0.05,\n",
    "   # 'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# Train the LightGBM model with MAPE as the evaluation metric\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    num_boost_round=1000,\n",
    "    feval=mape\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain gain feature importance\n",
    "gain_importance = model.feature_importance(importance_type='gain')\n",
    "\n",
    "# Display feature importance with feature names\n",
    "feature_names = model.feature_name()\n",
    "gain_importance_df = pd.DataFrame({'Feature': feature_names, 'Gain': gain_importance})\n",
    "gain_importance_df=gain_importance_df.sort_values(by='Gain', ascending=False)รง\n",
    "\n",
    "# Obtain split feature importance\n",
    "split_importance = model.feature_importance(importance_type='split')\n",
    "\n",
    "# Display feature importance\n",
    "feature_names = model.feature_name()\n",
    "split_importance_df = pd.DataFrame({'Feature': feature_names, 'Split': split_importance})\n",
    "split_importance_df=split_importance_df.sort_values(by='Split', ascending=False)\n",
    "\n",
    "unique_features = list(set(split_importance_df.Feature[:500]).union(set(gain_importance_df.Feature[:500])))\n",
    "unique_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[unique_features]\n",
    "X_test=X_test[unique_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming X_train, X_test, Y_train_sc, Y_test_sc are defined\n",
    "\n",
    "# Define the number of trials\n",
    "n_trials = 20\n",
    "\n",
    "# Define a list of random seeds\n",
    "seeds = [79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157,\n",
    "         163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241,\n",
    "         251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349,\n",
    "         353, 359, 367, 373, 379, 383, 389, 397]\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Create a list to store successful trials\n",
    "successful_trials = []\n",
    "\n",
    "def objective(trial):\n",
    "    # Select a random seed for this trial\n",
    "    seed = np.random.choice(seeds)\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = X_train, X_test, Y_train_sc, Y_test_sc\n",
    "    param = {\n",
    "        'metric': 'rmse', \n",
    "        'random_state': seed,  # Use the selected seed\n",
    "        'n_estimators': 20000,\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.5, 0.4, 0.7, 1]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 6, 9,12,16]),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 256),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300)\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(**param, verbose=-1)  \n",
    "    \n",
    "    try:\n",
    "        model.fit(train_x, train_y, eval_set=[(test_x, test_y)])  # Print every 500 iterations\n",
    "        preds = model.predict(test_x)\n",
    "        rmse = mean_squared_error(test_y, preds, squared=False)\n",
    "        \n",
    "        # Save the results for each trial\n",
    "        results.append({'seed': seed, 'hyperparameters': param, 'rmse': rmse})\n",
    "        \n",
    "        # Save the model with a unique identifier\n",
    "        model_name = f\"/home/all4data118/buckets/b1/exp/model_es_b/client_product/model_trial_{trial.number}_seed_{seed}_rmse_{rmse:.4f}.joblib\"\n",
    "        joblib.dump(model, model_name)\n",
    "        \n",
    "        # Append trial parameters to successful_trials list\n",
    "        successful_trials.append(param)\n",
    "        \n",
    "        return rmse\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        return float('inf')  # Return a large value to indicate failure\n",
    "        \n",
    "# Create a single study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Optimize for the specified number of trials\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "# Print the best trial parameters\n",
    "print('Best trial parameters:')\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# Print the results for each trial\n",
    "print('Results for each trial:')\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# Print successful trial parameters\n",
    "print('Successful trial parameters:')\n",
    "for params in successful_trials:\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().to_csv('/home/all4data118/buckets/b1/exp/model_es_b/client_product/trials_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('/home/all4data118/buckets/b1/exp/model_es_b/client_product/results_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Create an empty DataFrame to store the inverse transformed values\n",
    "inverse_transformed_df = pd.DataFrame()\n",
    "grouped = y_test_df.groupby('product_id')['tn_mas_2_scaled']\n",
    "column_to_standardize = 'tn_mas_2_scaled'\n",
    "\n",
    "\n",
    "# Load the scaler object from the file\n",
    "scalers = joblib.load('scalers_product_tn.joblib')\n",
    "\n",
    "# Inverse transform each group and concatenate the results\n",
    "for group_name, group_data in grouped:\n",
    "    # Extract the values to be inverse transformed\n",
    "    values_to_inverse_transform = y_test_df.loc[group_data.index][column_to_standardize].values.reshape(-1, 1)\n",
    "\n",
    "    # Retrieve the scaler for this group from the dictionary\n",
    "    scaler = scalers[group_name]\n",
    "\n",
    "    # Inverse transform the data using the original mean and standard deviation\n",
    "    inverse_transformed_values = scaler.inverse_transform(values_to_inverse_transform)\n",
    "\n",
    "    # Create a DataFrame with the inverse transformed values and 'product_id' as the index\n",
    "    group_df = pd.DataFrame(inverse_transformed_values, index=group_data.index, columns=[column_to_standardize])\n",
    "\n",
    "    # Concatenate the group DataFrame to the overall inverse transformed DataFrame\n",
    "    inverse_transformed_df = pd.concat([inverse_transformed_df, group_df])\n",
    "\n",
    "# Sort the DataFrame by index to get the original order\n",
    "y_test_df_transformed = inverse_transformed_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Create an empty DataFrame to store the inverse transformed values\n",
    "inverse_transformed_df = pd.DataFrame()\n",
    "grouped = y_pred_df.groupby('product_id')['y_pred']\n",
    "column_to_standardize = 'y_pred'\n",
    "\n",
    "\n",
    "# Load the scaler object from the file\n",
    "scalers = joblib.load('scalers_product_tn.joblib')\n",
    "\n",
    "# Inverse transform each group and concatenate the results\n",
    "for group_name, group_data in grouped:\n",
    "    # Extract the values to be inverse transformed\n",
    "    values_to_inverse_transform = y_pred_df.loc[group_data.index][column_to_standardize].values.reshape(-1, 1)\n",
    "\n",
    "    # Retrieve the scaler for this group from the dictionary\n",
    "    scaler = scalers[group_name]\n",
    "\n",
    "    # Inverse transform the data using the original mean and standard deviation\n",
    "    inverse_transformed_values = scaler.inverse_transform(values_to_inverse_transform)\n",
    "\n",
    "    # Create a DataFrame with the inverse transformed values and 'product_id' as the index\n",
    "    group_df = pd.DataFrame(inverse_transformed_values, index=group_data.index, columns=[column_to_standardize])\n",
    "\n",
    "    # Concatenate the group DataFrame to the overall inverse transformed DataFrame\n",
    "    inverse_transformed_df = pd.concat([inverse_transformed_df, group_df])\n",
    "\n",
    "# Sort the DataFrame by index to get the original order\n",
    "y_pred_df_transformed = inverse_transformed_df.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df=pd.DataFrame({'product_id':df[(df.periodo==201902)]['product_id'].values,'tn_mas_2_scaled':df[(df.periodo==201902)]['tn_mas_2_scaled'].values,'tn_mas_2_real':df[(df.periodo==201902)]['tn_mas_2'].values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Create an empty DataFrame to store the inverse transformed values\n",
    "inverse_transformed_df = pd.DataFrame()\n",
    "grouped = y_test_df.groupby('product_id')['tn_mas_2_scaled']\n",
    "column_to_standardize = 'tn_mas_2_scaled'\n",
    "\n",
    "\n",
    "# Load the scaler object from the file\n",
    "scalers = joblib.load('scalers_product_tn.joblib')\n",
    "\n",
    "# Inverse transform each group and concatenate the results\n",
    "for group_name, group_data in grouped:\n",
    "    # Extract the values to be inverse transformed\n",
    "    values_to_inverse_transform = y_test_df.loc[group_data.index][column_to_standardize].values.reshape(-1, 1)\n",
    "\n",
    "    # Retrieve the scaler for this group from the dictionary\n",
    "    scaler = scalers[group_name]\n",
    "\n",
    "    # Inverse transform the data using the original mean and standard deviation\n",
    "    inverse_transformed_values = scaler.inverse_transform(values_to_inverse_transform)\n",
    "\n",
    "    # Create a DataFrame with the inverse transformed values and 'product_id' as the index\n",
    "    group_df = pd.DataFrame(inverse_transformed_values, index=group_data.index, columns=[column_to_standardize])\n",
    "\n",
    "    # Concatenate the group DataFrame to the overall inverse transformed DataFrame\n",
    "    inverse_transformed_df = pd.concat([inverse_transformed_df, group_df])\n",
    "\n",
    "# Sort the DataFrame by index to get the original order\n",
    "y_test_df_transformed = inverse_transformed_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=y_pred_df_transformed.join(y_test_df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['diffe']=np.abs(total['y_pred']-total['tn_mas_2_scaled'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
