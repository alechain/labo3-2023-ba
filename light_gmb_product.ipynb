{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_mas_cols = df.filter(like='tn_mas_1').columns\n",
    "print(tn_mas_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.periodo.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to numerical format\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = pd.Categorical(df[col]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/home/all4data118/buckets/b1/datasets/df_sellout_prod_train_fe_es_b.parquet')\n",
    "tn_mas_cols = df.filter(like='tn_mas_2').columns\n",
    "print(tn_mas_cols)\n",
    "# train y test\n",
    "X_train=df[(df.periodo>=201801)&(df.periodo<=201908)]\n",
    "X_test=df[(df.periodo==201909)]\n",
    "Y_train=df[(df.periodo>=201801)&(df.periodo<=201908)][['id_row','tn_mas_2_scaled']]\n",
    "Y_test=df[(df.periodo==201909)][['id_row','tn_mas_2_scaled']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.drop(['id_row','tn_mas_2_scaled','tn_mas_2'],axis=1)\n",
    "X_test=X_test.drop(['id_row','tn_mas_2_scaled','tn_mas_2'],axis=1)\n",
    "\n",
    "Y_train_sc=Y_train[['tn_mas_2_scaled']]\n",
    "Y_test_sc=Y_test[['tn_mas_2_scaled']]\n",
    "# Ensure that the number of labels matches the number of data points\n",
    "assert len(X_train) == len(Y_train_sc), \"Mismatch in the number of labels for training set\"\n",
    "assert len(X_test) == len(Y_test_sc), \"Mismatch in the number of labels for validation set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features based on their data types\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    X_train[feature] = label_encoder.fit_transform(X_train[feature])\n",
    "    X_test[feature] = label_encoder.transform(X_test[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have your data in X_train, Y_train_sc, X_test, and Y_test_sc\n",
    "\n",
    "# Define your custom metric function (MAPE)\n",
    "def mape(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    absolute_percentage_errors = np.abs((y_true - y_pred) / y_true)\n",
    "    return 'MAPE', np.mean(absolute_percentage_errors) * 100, False\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=Y_train['tn_mas_2_scaled'], categorical_feature=categorical_features)\n",
    "val_data = lgb.Dataset(X_test, label=Y_test['tn_mas_2_scaled'], categorical_feature=categorical_features)\n",
    "\n",
    "# Define LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "  #  'metric': 'l1',  # Using 'l1' as the default metric for early stopping\n",
    "   # 'boosting_type': 'gbdt',\n",
    "   # 'num_leaves': 31,\n",
    "   # 'learning_rate': 0.05,\n",
    "   # 'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# Train the LightGBM model with MAPE as the evaluation metric\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    num_boost_round=1000,\n",
    "    feval=mape\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain gain feature importance\n",
    "gain_importance = model.feature_importance(importance_type='gain')\n",
    "\n",
    "# Display feature importance with feature names\n",
    "feature_names = model.feature_name()\n",
    "gain_importance_df = pd.DataFrame({'Feature': feature_names, 'Gain': gain_importance})\n",
    "gain_importance_df=gain_importance_df.sort_values(by='Gain', ascending=False)รง\n",
    "\n",
    "# Obtain split feature importance\n",
    "split_importance = model.feature_importance(importance_type='split')\n",
    "\n",
    "# Display feature importance\n",
    "feature_names = model.feature_name()\n",
    "split_importance_df = pd.DataFrame({'Feature': feature_names, 'Split': split_importance})\n",
    "split_importance_df=split_importance_df.sort_values(by='Split', ascending=False)\n",
    "\n",
    "unique_features = list(set(split_importance_df.Feature[:500]).union(set(gain_importance_df.Feature[:500])))\n",
    "unique_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[unique_features]\n",
    "X_test=X_test[unique_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Assuming X_train, X_test, Y_train_sc, Y_test_sc are defined\n",
    "\n",
    "# Define the number of trials\n",
    "n_trials = 50\n",
    "\n",
    "# Define a list of random seeds\n",
    "seeds = [ 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157,\n",
    "          163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, \n",
    "          251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, \n",
    "          353, 359, 367, 373, 379, 383, 389, 397]\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "def objective(trial):\n",
    "    # Select a random seed for this trial\n",
    "    seed = np.random.choice(seeds)\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = X_train, X_test, Y_train_sc, Y_test_sc\n",
    "    param = {\n",
    "        'metric': 'rmse', \n",
    "        'random_state': seed,  # Use the selected seed\n",
    "        'n_estimators': 20000,\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 1.0]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.5, 0.4, 0.7, 1]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 6, 9,12,16]),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 256),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
    "        'cat_smooth': trial.suggest_int('cat_smooth', 1, 100),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_loguniform('min_sum_hessian_in_leaf', 0.1, 10)\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(**param)  \n",
    "    \n",
    "    model.fit(train_x, train_y, eval_set=[(test_x, test_y)], verbose=False)\n",
    "    \n",
    "    preds = model.predict(test_x)\n",
    "    \n",
    "    rmse = mean_squared_error(test_y, preds, squared=False)\n",
    "    \n",
    "    # Save the results for each trial\n",
    "    results.append({'seed': seed, 'hyperparameters': param, 'rmse': rmse})\n",
    "    \n",
    "    # Save the model with a unique identifier\n",
    "    model_name = f\"'/home/all4data118/exp/model_es_a/product/model_trial_{trial.number}_seed_{seed}_rmse_{rmse:.4f}.joblib\"\n",
    "    joblib.dump(model, model_name)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Create a list to store the best trial parameters\n",
    "best_trial_params = []\n",
    "\n",
    "# Iterate through trials\n",
    "for _ in range(n_trials):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=1)\n",
    "    best_trial_params.append(study.best_trial.params)\n",
    "\n",
    "# Print the best trial parameters\n",
    "print('Best trial parameters:')\n",
    "for params in best_trial_params:\n",
    "    print(params)\n",
    "\n",
    "# Print the results for each trial\n",
    "print('Results for each trial:')\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Create an empty DataFrame to store the inverse transformed values\n",
    "inverse_transformed_df = pd.DataFrame()\n",
    "grouped = y_test_df.groupby('product_id')['tn_mas_2_scaled']\n",
    "column_to_standardize = 'tn_mas_2_scaled'\n",
    "\n",
    "\n",
    "# Load the scaler object from the file\n",
    "scalers = joblib.load('scalers_product_tn.joblib')\n",
    "\n",
    "# Inverse transform each group and concatenate the results\n",
    "for group_name, group_data in grouped:\n",
    "    # Extract the values to be inverse transformed\n",
    "    values_to_inverse_transform = y_test_df.loc[group_data.index][column_to_standardize].values.reshape(-1, 1)\n",
    "\n",
    "    # Retrieve the scaler for this group from the dictionary\n",
    "    scaler = scalers[group_name]\n",
    "\n",
    "    # Inverse transform the data using the original mean and standard deviation\n",
    "    inverse_transformed_values = scaler.inverse_transform(values_to_inverse_transform)\n",
    "\n",
    "    # Create a DataFrame with the inverse transformed values and 'product_id' as the index\n",
    "    group_df = pd.DataFrame(inverse_transformed_values, index=group_data.index, columns=[column_to_standardize])\n",
    "\n",
    "    # Concatenate the group DataFrame to the overall inverse transformed DataFrame\n",
    "    inverse_transformed_df = pd.concat([inverse_transformed_df, group_df])\n",
    "\n",
    "# Sort the DataFrame by index to get the original order\n",
    "y_test_df_transformed = inverse_transformed_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Create an empty DataFrame to store the inverse transformed values\n",
    "inverse_transformed_df = pd.DataFrame()\n",
    "grouped = y_pred_df.groupby('product_id')['y_pred']\n",
    "column_to_standardize = 'y_pred'\n",
    "\n",
    "\n",
    "# Load the scaler object from the file\n",
    "scalers = joblib.load('scalers_product_tn.joblib')\n",
    "\n",
    "# Inverse transform each group and concatenate the results\n",
    "for group_name, group_data in grouped:\n",
    "    # Extract the values to be inverse transformed\n",
    "    values_to_inverse_transform = y_pred_df.loc[group_data.index][column_to_standardize].values.reshape(-1, 1)\n",
    "\n",
    "    # Retrieve the scaler for this group from the dictionary\n",
    "    scaler = scalers[group_name]\n",
    "\n",
    "    # Inverse transform the data using the original mean and standard deviation\n",
    "    inverse_transformed_values = scaler.inverse_transform(values_to_inverse_transform)\n",
    "\n",
    "    # Create a DataFrame with the inverse transformed values and 'product_id' as the index\n",
    "    group_df = pd.DataFrame(inverse_transformed_values, index=group_data.index, columns=[column_to_standardize])\n",
    "\n",
    "    # Concatenate the group DataFrame to the overall inverse transformed DataFrame\n",
    "    inverse_transformed_df = pd.concat([inverse_transformed_df, group_df])\n",
    "\n",
    "# Sort the DataFrame by index to get the original order\n",
    "y_pred_df_transformed = inverse_transformed_df.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df=pd.DataFrame({'product_id':df[(df.periodo==201902)]['product_id'].values,'tn_mas_2_scaled':df[(df.periodo==201902)]['tn_mas_2_scaled'].values,'tn_mas_2_real':df[(df.periodo==201902)]['tn_mas_2'].values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Create an empty DataFrame to store the inverse transformed values\n",
    "inverse_transformed_df = pd.DataFrame()\n",
    "grouped = y_test_df.groupby('product_id')['tn_mas_2_scaled']\n",
    "column_to_standardize = 'tn_mas_2_scaled'\n",
    "\n",
    "\n",
    "# Load the scaler object from the file\n",
    "scalers = joblib.load('scalers_product_tn.joblib')\n",
    "\n",
    "# Inverse transform each group and concatenate the results\n",
    "for group_name, group_data in grouped:\n",
    "    # Extract the values to be inverse transformed\n",
    "    values_to_inverse_transform = y_test_df.loc[group_data.index][column_to_standardize].values.reshape(-1, 1)\n",
    "\n",
    "    # Retrieve the scaler for this group from the dictionary\n",
    "    scaler = scalers[group_name]\n",
    "\n",
    "    # Inverse transform the data using the original mean and standard deviation\n",
    "    inverse_transformed_values = scaler.inverse_transform(values_to_inverse_transform)\n",
    "\n",
    "    # Create a DataFrame with the inverse transformed values and 'product_id' as the index\n",
    "    group_df = pd.DataFrame(inverse_transformed_values, index=group_data.index, columns=[column_to_standardize])\n",
    "\n",
    "    # Concatenate the group DataFrame to the overall inverse transformed DataFrame\n",
    "    inverse_transformed_df = pd.concat([inverse_transformed_df, group_df])\n",
    "\n",
    "# Sort the DataFrame by index to get the original order\n",
    "y_test_df_transformed = inverse_transformed_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=y_pred_df_transformed.join(y_test_df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['diffe']=np.abs(total['y_pred']-total['tn_mas_2_scaled'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
